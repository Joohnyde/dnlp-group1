# ðŸ§  LoRA: Reproduction and Extension Challenge

This repository contains the work of **Group 1** for the **Reproducibility and Extension Challenge** assigned in the course:

> **192.039 Deep Natural Language Processing**  
> Vienna University of Technology (TU Wien) Â· Summer Semester 2025

We focus on the paper:

> **LoRA: Low-Rank Adaptation of Large Language Models**  
> Edward Hu, Yelong Shen, Phillip Wallis, et al.  
> [arXiv:2106.09685](https://arxiv.org/abs/2106.09685)

---

## ðŸŽ¯ Project Objectives

The goal of this project was to:
- ðŸ“– **Understand** the motivation and mechanics of LoRA.
- ðŸ” **Reproduce** a key experiment from the paper (RoBERTa fine-tuning on SST-2).
- ðŸ§ª **Extend** the original work by testing LoRA on additional setups and parameters.
- ðŸ§‘â€ðŸ« **Present** our findings in a detailed presentation and participate in group interviews.

---

## ðŸ‘¥ Group Members

| Name              | Student ID   |
|-------------------|--------------|
| Denijal Sendelj   | 12129474     |
| Richard Keil      | 12447214     |
| Thomas Ranner     | 11771213     |

---

## ðŸ“Œ Summary of the Paper

**LoRA** introduces a simple, scalable technique for efficient fine-tuning of pre-trained large language models (LLMs).  
Instead of updating all model parameters, LoRA freezes them and injects trainable low-rank matrices into specific layers (e.g., attention projections). This reduces the number of trainable parameters by orders of magnitude, while achieving comparable performance to full fine-tuning.

Key contributions:
- Efficient parameter-efficient fine-tuning (PEFT).
- Strong empirical results across several models (RoBERTa, GPT-2, etc.).
- Reusable and model-agnostic method.

---

## ðŸ› ï¸ Codebase Contents

dnlp-group1/ 

â”œâ”€â”€ reproduction/ # Reproduction of the paper's experiment 

â”‚ â”œâ”€â”€ train_baseline.py # Full fine-tuning (baseline) 

â”‚ â”œâ”€â”€ train_lora.py # LoRA fine-tuning using PEFT 

â”‚ â””â”€â”€ utils.py

â”œâ”€â”€ extension/ # Extensions beyond the original paper 

â”‚ â”œâ”€â”€ alt_task_mnli.py # LoRA on a new GLUE task (e.g. MNLI) 

â”‚ â”œâ”€â”€ low_rank_analysis.py # Varying LoRA rank and analyzing performance 

â”‚ â””â”€â”€ no_attention_test.py # Experimenting with non-standard modules 

â”œâ”€â”€ plots/ # Accuracy, parameter count, training graphs 

â”œâ”€â”€ notebooks/ # Exploratory or evaluation notebooks 

â”œâ”€â”€ presentation/ # Final PowerPoint slides 

â”‚ â””â”€â”€ LoRA_TUW_Group1.pdf 

â”œâ”€â”€ requirements.txt # Python dependencies 

â”œâ”€â”€ README.md # This file 

â””â”€â”€ LICENSE

---

## ðŸ” Reproduction

We reproduced the fine-tuning of **RoBERTa-base** on the **SST-2** sentiment classification task, comparing:

- **Full fine-tuning** (all weights updated)
- **LoRA fine-tuning** (only low-rank matrices updated)

We used:
- HuggingFace Transformers
- HuggingFace Datasets (GLUE benchmark)
- PEFT library from HuggingFace for LoRA integration

ðŸ“Š Results:

| Method           | Accuracy (SST-2) | Trainable Params | Training Time |
|------------------|------------------|------------------|----------------|
| Full Fine-Tuning | 93.2%            | ~125M            | ~12 min        |
| LoRA (r=8)       | 92.9%            | ~1.8M            | ~9 min         |

âœ… LoRA nearly matches full fine-tuning performance with only ~1.5% of the trainable parameters.

---

## ðŸ§ª Extensions

We conducted several novel experiments to extend the findings of the paper:

### ðŸ”¹ 1. LoRA on a Different Task: MNLI
We tested LoRA on a more complex GLUE task â€“ **Multi-Genre Natural Language Inference (MNLI)** â€“ to see how it generalizes beyond SST-2.

### ðŸ”¹ 2. LoRA Rank Sweep
We ran a series of experiments with different **LoRA ranks (r = 1, 2, 4, 8, 16)** to observe the trade-off between parameter count and accuracy.

### ðŸ”¹ 3. Non-Attention Target Modules (Exploratory)
We tried applying LoRA to modules **outside of attention**, such as feed-forward layers, to test how sensitive the technique is to where itâ€™s applied.

All extensions are fully documented in the `extension/` folder and discussed in the final presentation.

---

## ðŸ“½ï¸ Presentation

You can find our final presentation slides here:  
ðŸ“„ [`presentation/LoRA_TUW_Group1.pdf`](./presentation/LoRA_TUW_Group1.pdf)

Presentation structure:
1. **Introduction** â€“ Problem space, what LoRA is
2. **Understanding** â€“ Theory and intuition
3. **Reproduction** â€“ Code, setup, results
4. **Extensions** â€“ Our new experiments
5. **Conclusion** â€“ Takeaways, reproducibility notes, future work

---

## ðŸ§  Individual Contributions

- **Denijal Sendelj**: Reproduced SST-2 experiments, implemented full vs. LoRA runs, plotted training graphs.
- **Richard Keil**: Designed and implemented LoRA-on-MNLI extension and parameter sweep.
- **Thomas Ranner**: Handled theoretical explanation, slides, and exploratory experiments on LoRA in feed-forward layers.

Each team member also participated in group debugging, syncs, and presentation preparation.

---

## ðŸ”— Acknowledgments

- PEFT Library: https://github.com/huggingface/peft  
- Transformers: https://github.com/huggingface/transformers  
- Datasets: https://huggingface.co/datasets/glue

> All reused code is acknowledged and modified appropriately. Our extensions go beyond publicly available implementations.

---

## ðŸ“œ License

This project is licensed under the MIT License.  
See [LICENSE](./LICENSE) for details.

---

